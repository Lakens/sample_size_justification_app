---
title: "Examples of specifying a smallest effect size of interest"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## From https://rr.peercommunityin.org/articles/rec?id=144

This sample size justification combines resource limitations and a smallest effect size of interest. 

Our planned sample size is informed by the effect sizes obtained from Kelly et al. (2021) and Rundle et al. (2021). For our main effects of interest (see “Vignette development” below), Kelly et al. observed a significant effect of Cohen’s ds ~ .15 for perceived danger, ds ~ .20 for prognostic optimism, ds ~ .30 for continuing care and ds ~ .43 for blame, whilst Rundle et al. observed an effect of ds ~ .1.03 for Stigma Ratings . We conducted a series of sensitivity power analyses based on the two one-sided tests procedure for equivalence testing (see Dienes, 2021; Lakens, 2017). In the first, we input the smallest significant effect of −ΔL = -.15 and ΔU = .15 from Kelly et al., which requires 2,804 participants to achieve 90% statistical power with alpha set at .01. However, this is outside of our funding resources (see Lakens et al., 2021). For this reason, we then input the second smallest effect of −ΔL = -.20 and ΔU = .20, again from Kelly et al., which requires 1,578 participants (n  = 789 per factor): given that this is within our resources, this determined our planned sample size. Note that effect sizes of  ds =/> .20 have also been found in meta-analyses assessing the influence of the brain disease model on public stigma (Kvaale et al., 2013) meaning that the planned sample size would yield informative results with respect to the presence or absence of effect size estimates provided by this meta-analysis. 


## From http://dx.doi.org/10.1111/lcrp.12186

*The example below uses the small telescopes approach proposed by Simonsohn (2015) for direct replication studies.*

The sample size for the direct replication is determined based on an a priori power analysis. Relying on the effect sizes of the original study is not recommended when designing replication studies, as this leads to underpowered replications (Albers & Lakens, 2018; Simonsohn, 2015). We therefore determined first the smallest effect size of interest (SESOI; Lakens,Scheel,& Isager, 2018) by following Simonsohn’s (2015) advise to consider the effect size that would give the original study 33% power. The SESOI is determined as f2 = 0.016 - a small effect. Conducting an a-priori power analysis for multiple regressions - a=.05, power=.95 - then yields a target sample size of N = 1,378.

## From http://dx.doi.org/10.1037/pro0000386

Several decades ago Meehl (1991) suggested that many small findings in psychology could be “crud,” a tendency for everything to correlate with everything else a tiny amount. The current article sought to provide some preliminary examination of the crud phenomenon-in a large sample of adolescents with aggressive delinquency as an outcome. Analyses from Study 1 suggest that with effect sizes below r = .10, a majority of nonsense relationships achieve statistical significance, with some approaching or slightly exceeding the value of r = .10. These results suggest a higher than tolerable probability for false-positive findings among effect sizes
below r = .10 among large sample studies. It is interesting that some of these variables produced effect sizes higher than for screen use, a variable often considered important by psychologists when considering aggression. This highlights the potential for spurious interpretation of weak effects.

With Study 2, mean effect sizes were slightly higher, around r = .11. This suggests that even some effect sizes exceeding r = .11 and “statistically significant” in large datasets may be artifactual in nature. Though there is no definite hard “cut-off” effect size,
confidence in the meaningfulness of statistically significant effect sizes should clearly decrease the nearer they approximate 0. In the two datasets, only a single crud relationship exceeded r = .20. As a practical suggestion, it is recommended that effect sizes
below r = .10 should not be interpreted as evidence in support of a hypothesis, at least of one claiming the existence of a univariable relationship or a univariable causal effect.


## From https://osf.io/kd4ax

Our sample size estimate was based upon identifying a smallest effect size of interest that would indicate that one intervention produced meaningfully more fat mass loss than the other. That is to say the between condition treatment effect was of a magnitude that is considered a meaningful fat loss. The ACSM position stand suggests a 3% body mass loss as the smallest effect that leads to health improvements. Further, we expected this might be a reasonable magnitude of effect given our interventions as previous research has reported ~3.3% absolute fat mass loss for resistance exercise and similar dietary intervention over a one month period (Miller et al., 2017). 